{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "import csv\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "configFile = '../pipeline/config.yml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "SPARQL = 0\n",
    "CSV = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open(configFile, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "except:\n",
    "    raise Exception(\"Could not load config file at\", configFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparqlResultToDict(results):\n",
    "    rows = []\n",
    "    for result in results[\"results\"][\"bindings\"]:\n",
    "        row = {}\n",
    "        for key in results[\"head\"][\"vars\"]:\n",
    "            if key in result:\n",
    "                row[key] = result[key][\"value\"]\n",
    "            else:\n",
    "                row[key] = None\n",
    "        rows.append(row)\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Get input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = False\n",
    "if config['mode'] == \"SPARQL\":\n",
    "    mode = SPARQL\n",
    "elif config['mode'] == \"CSV\":\n",
    "    mode  = CSV\n",
    "else:\n",
    "    raise Exception(\"mode not specified or invalid (should be SPARQL or CSV)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read data from input file, if present. This is being done for both CSV and SPARQL mode as the SPARQL results will be cashed in the CSV file and updated when data is changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputData = []\n",
    "try:\n",
    "    with open(config['dataFile'], 'r') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            inputData.append({\n",
    "                \"id\": row['id'],\n",
    "                \"image\": row['image'],\n",
    "                \"width\": row['width'],\n",
    "                \"height\": row['height'],\n",
    "            })\n",
    "except:\n",
    "    print(\"No prior input file found\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If in SPARQL mode, get data from SPARQL endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mode == SPARQL:\n",
    "    if not config['endpoint'] or not config['query']:\n",
    "        raise Exception(\"incomplete configuration for SPARQL mode\")\n",
    "        \n",
    "    sparql = SPARQLWrapper(config['endpoint'], returnFormat=JSON)\n",
    "    sparql.setQuery(config['query'])\n",
    "    try:\n",
    "        ret = sparql.query().convert()\n",
    "    except:\n",
    "        raise Exception(\"Could not execute query against endpoint\", config['endpoint'])\n",
    "    queriedData = sparqlResultToDict(ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If in SPARQL mode, merge queried data with data stored in CSV file.\n",
    "- add entries that exist in SPARQL result, but not in the CSV file\n",
    "- add width/height information when it is only available in either the CSV file or the SPARQL output (prioritising the SPARQL data)\n",
    "Store merged data in CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-9-6fa970eada8e>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-9-6fa970eada8e>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    if mode == SPARQL\u001b[0m\n\u001b[0m                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "if mode == SPARQL:\n",
    "    inputDataHash = {}\n",
    "    queriedDataHash = {}\n",
    "\n",
    "    for row in inputData:\n",
    "        inputDataHash[row['id']] = row\n",
    "    for row in queriedData:\n",
    "        queriedDataHash[row['id']] = row\n",
    "\n",
    "    data = inputData\n",
    "    idsInInputData = [d['id'] for d in inputData]\n",
    "    for row in queriedData:\n",
    "        if row['id'] not in idsInInputData:\n",
    "            data.append(row)\n",
    "\n",
    "    for row in data:\n",
    "        if not row['width']:\n",
    "            if row['id'] in queriedDataHash and queriedDataHash[row['id']]['width']:\n",
    "                row['width'] = queriedDataHash[row['id']]['width']\n",
    "            elif row['id'] in inputDataHash and inputDataHash[row['id']]['width']:\n",
    "                row['width'] = inputDataHash[row['id']]['width']\n",
    "        if not row['height']:\n",
    "            if row['id'] in queriedDataHash and queriedDataHash[row['id']]['height']:\n",
    "                row['height'] = queriedDataHash[row['id']]['height']\n",
    "            elif row['id'] in inputDataHash and inputDataHash[row['id']]['width']:\n",
    "                row['height'] = inputDataHash[row['id']]['height']\n",
    "                \n",
    "    try:\n",
    "        with open(config['dataFile'], 'w') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=['id','image','width','height'])\n",
    "            writer.writeheader()\n",
    "            for row in data:\n",
    "                writer.writerow(row)\n",
    "    except:\n",
    "        raise Exception(\"Could not write to\", config['dataFile'])\n",
    "else:\n",
    "    data = \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
