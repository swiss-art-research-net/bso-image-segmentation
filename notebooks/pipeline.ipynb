{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import urllib.request\n",
    "import requests\n",
    "import uuid\n",
    "import time\n",
    "import yaml\n",
    "from os import path\n",
    "from pathlib import Path\n",
    "from string import Template\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "configFile = '../pipeline/config.yml'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open(configFile, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "except:\n",
    "    raise Exception(\"Could not load config file at\", configFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "SPARQL = 0\n",
    "CSV = 1\n",
    "\n",
    "def sparqlResultToDict(results):\n",
    "    rows = []\n",
    "    for result in results[\"results\"][\"bindings\"]:\n",
    "        row = {}\n",
    "        for key in results[\"head\"][\"vars\"]:\n",
    "            if key in result:\n",
    "                row[key] = result[key][\"value\"]\n",
    "            else:\n",
    "                row[key] = None\n",
    "        rows.append(row)\n",
    "    return rows\n",
    "\n",
    "def writeData(data):\n",
    "    try:\n",
    "        with open(config['dataFile'], 'w') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=['id','image','width','height','documentCoordinates'])\n",
    "            writer.writeheader()\n",
    "            for row in data:\n",
    "                if not 'documentCoordinates' in row:\n",
    "                    row['documentCoordinates'] = None\n",
    "                writer.writerow(row)\n",
    "    except:\n",
    "        raise Exception(\"Could not write to\", config['dataFile'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Get input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = False\n",
    "if config['mode'] == \"SPARQL\":\n",
    "    mode = SPARQL\n",
    "elif config['mode'] == \"CSV\":\n",
    "    mode  = CSV\n",
    "else:\n",
    "    raise Exception(\"mode not specified or invalid (should be SPARQL or CSV)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read data from input file, if present. This is being done for both CSV and SPARQL mode as the SPARQL results will be cashed in the CSV file and updated when data is changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputData = []\n",
    "try:\n",
    "    with open(config['dataFile'], 'r') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            inputData.append({\n",
    "                \"id\": row['id'],\n",
    "                \"image\": row['image'],\n",
    "                \"width\": row['width'],\n",
    "                \"height\": row['height'],\n",
    "                \"documentCoordinates\": row['documentCoordinates'] if 'documentCoordinates' in row else None\n",
    "            })\n",
    "except:\n",
    "    print(\"No prior input file found\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If in SPARQL mode, get data from SPARQL endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mode == SPARQL:\n",
    "    if not config['endpoint'] or not config['query']:\n",
    "        raise Exception(\"incomplete configuration for SPARQL mode\")\n",
    "        \n",
    "    sparql = SPARQLWrapper(config['endpoint'], returnFormat=JSON)\n",
    "    sparql.setQuery(config['query'])\n",
    "    try:\n",
    "        ret = sparql.query().convert()\n",
    "    except:\n",
    "        raise Exception(\"Could not execute query against endpoint\", config['endpoint'])\n",
    "    queriedData = sparqlResultToDict(ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If in SPARQL mode, merge queried data with data stored in CSV file.\n",
    "- add entries that exist in SPARQL result, but not in the CSV file\n",
    "- add width/height information when it is only available in either the CSV file or the SPARQL output (prioritising the SPARQL data)\n",
    "Store merged data in CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = inputData\n",
    "\n",
    "if mode == SPARQL:\n",
    "    inputDataHash = {}\n",
    "    queriedDataHash = {}\n",
    "\n",
    "    for row in inputData:\n",
    "        inputDataHash[row['id']] = row\n",
    "    for row in queriedData:\n",
    "        queriedDataHash[row['id']] = row\n",
    "\n",
    "    idsInInputData = [d['id'] for d in inputData]\n",
    "    for row in queriedData:\n",
    "        if row['id'] not in idsInInputData:\n",
    "            data.append(row)\n",
    "\n",
    "    for row in data:\n",
    "        if not row['width']:\n",
    "            if row['id'] in queriedDataHash and queriedDataHash[row['id']]['width']:\n",
    "                row['width'] = queriedDataHash[row['id']]['width']\n",
    "            elif row['id'] in inputDataHash and inputDataHash[row['id']]['width']:\n",
    "                row['width'] = inputDataHash[row['id']]['width']\n",
    "        if not row['height']:\n",
    "            if row['id'] in queriedDataHash and queriedDataHash[row['id']]['height']:\n",
    "                row['height'] = queriedDataHash[row['id']]['height']\n",
    "            elif row['id'] in inputDataHash and inputDataHash[row['id']]['width']:\n",
    "                row['height'] = inputDataHash[row['id']]['height']\n",
    "    \n",
    "    writeData(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Get (missing) image sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the original image size is not specified, call the IIIF Image API to read the size from the JSON rsponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28103/28103 [00:00<00:00, 269048.40it/s]\n"
     ]
    }
   ],
   "source": [
    "for row in tqdm(data):\n",
    "    if not row['width'] or not row['height']:\n",
    "        uri = row['image'] + '/info.json'\n",
    "        try:\n",
    "            with urllib.request.urlopen(uri) as url:\n",
    "                manifestData = json.loads(url.read().decode())\n",
    "                \n",
    "        except:\n",
    "            print(\"Could not open\", uri)\n",
    "            next\n",
    "        row['width'] = manifestData['width']\n",
    "        row['height'] = manifestData['height']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write data to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "writeData(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Download images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the images that do not yet exist in the image folder. The images will be downloaded resized to a width of 1024 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    Path(config['imageDirectory']).mkdir(parents=True, exist_ok=True)\n",
    "except:\n",
    "    raise Exception(\"Could not add/access folder\", config['imageDirectory'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 24683/28103 [00:19<00:31, 107.22it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not download nb-812808\n",
      "Could not download nb-815037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 24744/28103 [00:30<01:42, 32.89it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not download nb-815050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 88%|████████▊ | 24747/28103 [00:35<02:33, 21.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not download nb-815054\n",
      "Could not download nb-815062\n",
      "Could not download nb-815093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 24763/28103 [00:52<05:55,  9.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not download nb-815097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 88%|████████▊ | 24766/28103 [00:58<07:29,  7.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not download nb-815102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 88%|████████▊ | 24832/28103 [01:04<06:28,  8.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not download nb-815125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 25005/28103 [01:10<03:09, 16.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not download nb-815670\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████▏| 25651/28103 [01:16<00:40, 60.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not download nb-822350\n",
      "Could not download nb-838092\n",
      "Could not download nb-838152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 25756/28103 [01:33<02:00, 19.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not download nb-838155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 92%|█████████▏| 25757/28103 [01:38<02:42, 14.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not download nb-838157\n",
      "Could not download nb-838160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 25759/28103 [01:50<04:41,  8.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not download nb-838162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 92%|█████████▏| 25760/28103 [01:55<06:05,  6.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not download nb-838164\n",
      "Could not download nb-838166\n",
      "Could not download nb-838168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 25763/28103 [02:13<12:06,  3.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not download nb-838170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 92%|█████████▏| 25764/28103 [02:18<14:44,  2.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not download nb-838172\n",
      "Could not download nb-838174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 25766/28103 [02:30<21:53,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not download nb-838176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 92%|█████████▏| 25767/28103 [02:35<26:41,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not download nb-838178\n",
      "Could not download nb-838180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 92%|█████████▏| 25795/28103 [02:47<20:59,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not download nb-838182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 92%|█████████▏| 25880/28103 [02:52<08:07,  4.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not download nb-841831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 92%|█████████▏| 25900/28103 [02:58<08:30,  4.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not download nb-841890\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 92%|█████████▏| 25959/28103 [03:04<06:15,  5.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not download nb-861242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 93%|█████████▎| 26035/28103 [03:10<04:29,  7.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not download nb-870419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 26166/28103 [03:16<02:16, 14.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not download nb-870422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28103/28103 [18:55<00:00, 24.74it/s] \n"
     ]
    }
   ],
   "source": [
    "maxRetries = 5\n",
    "for row in tqdm(data):\n",
    "    filename = path.join(config['imageDirectory'], row['id'] + '.jpg')\n",
    "    if not path.exists(filename):\n",
    "        url = row['image'] + '/full/1024,/0/default.jpg'\n",
    "        r = requests.get(url, allow_redirects = True)\n",
    "        retries = 1\n",
    "        while not 'image' in r.headers['Content-Type'] and retries <= maxRetries:\n",
    "            # Try again if no image comes back\n",
    "            time.sleep(1)\n",
    "            r = requests.get(url, allow_redirects = True)\n",
    "            retries += 1\n",
    "        if retries >= maxRetries:\n",
    "            print(\"Could not download\", row['id'])\n",
    "        else:\n",
    "            with open(filename, 'wb') as f:\n",
    "                f.write(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Apply model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the model. This step is based on the code provided in the DH Segment example at https://github.com/dhlab-epfl/dhSegment/blob/master/demo.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from imageio import imread, imsave\n",
    "from tqdm import tqdm\n",
    "\n",
    "from dh_segment.io import PAGE\n",
    "from dh_segment.inference import LoadedModel\n",
    "from dh_segment.post_processing import boxes_detection, binarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def page_make_binary_mask(probs: np.ndarray, threshold: float=-1) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Computes the binary mask of the detected Page from the probabilities outputed by network\n",
    "    :param probs: array with values in range [0, 1]\n",
    "    :param threshold: threshold between [0 and 1], if negative Otsu's adaptive threshold will be used\n",
    "    :return: binary mask\n",
    "    \"\"\"\n",
    "\n",
    "    mask = binarization.thresholding(probs, threshold)\n",
    "    mask = binarization.cleaning_binary(mask, kernel_size=5)\n",
    "    return mask\n",
    "\n",
    "\n",
    "def format_quad_to_string(quad):\n",
    "    \"\"\"\n",
    "    Formats the corner points into a string.\n",
    "    :param quad: coordinates of the quadrilateral\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    s = ''\n",
    "    for corner in quad:\n",
    "        s += '{},{},'.format(corner[0], corner[1])\n",
    "    return s[:-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelDir = '../pretrained_models/bso_model/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ../pretrained_models/bso_model/\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/dh_segment/inference/loader.py:51: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from ../pretrained_models/bso_model/variables/variables\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▊   | 19303/28103 [02:16<00:39, 224.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File does not exist: ../data/images/nb-812808.jpg\n",
      "File does not exist: ../data/images/nb-815037.jpg\n",
      "File does not exist: ../data/images/nb-815050.jpg\n",
      "File does not exist: ../data/images/nb-815054.jpg\n",
      "File does not exist: ../data/images/nb-815062.jpg\n",
      "File does not exist: ../data/images/nb-815093.jpg\n",
      "File does not exist: ../data/images/nb-815097.jpg\n",
      "File does not exist: ../data/images/nb-815102.jpg\n",
      "File does not exist: ../data/images/nb-815125.jpg\n",
      "File does not exist: ../data/images/nb-815670.jpg\n",
      "File does not exist: ../data/images/nb-822350.jpg\n",
      "File does not exist: ../data/images/nb-838092.jpg\n",
      "File does not exist: ../data/images/nb-838152.jpg\n",
      "File does not exist: ../data/images/nb-838155.jpg\n",
      "File does not exist: ../data/images/nb-838157.jpg\n",
      "File does not exist: ../data/images/nb-838160.jpg\n",
      "File does not exist: ../data/images/nb-838162.jpg\n",
      "File does not exist: ../data/images/nb-838164.jpg\n",
      "File does not exist: ../data/images/nb-838166.jpg\n",
      "File does not exist: ../data/images/nb-838168.jpg\n",
      "File does not exist: ../data/images/nb-838170.jpg\n",
      "File does not exist: ../data/images/nb-838172.jpg\n",
      "File does not exist: ../data/images/nb-838174.jpg\n",
      "File does not exist: ../data/images/nb-838176.jpg\n",
      "File does not exist: ../data/images/nb-838178.jpg\n",
      "File does not exist: ../data/images/nb-838180.jpg\n",
      "File does not exist: ../data/images/nb-838182.jpg\n",
      "File does not exist: ../data/images/nb-841831.jpg\n",
      "File does not exist: ../data/images/nb-841890.jpg\n",
      "File does not exist: ../data/images/nb-861242.jpg\n",
      "File does not exist: ../data/images/nb-870419.jpg\n",
      "File does not exist: ../data/images/nb-870422.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28103/28103 [49:12<00:00,  9.52it/s] \n"
     ]
    }
   ],
   "source": [
    "with tf.Session():\n",
    "    # Load model\n",
    "    m = LoadedModel(modelDir, predict_mode='filename')\n",
    "    for row in tqdm(data):\n",
    "        if not row['documentCoordinates'] or len(row['documentCoordinates']) == 0:\n",
    "            filename = path.join(config['imageDirectory'], row['id'] + '.jpg')\n",
    "            if not path.isfile(filename):\n",
    "                print(\"File does not exist:\", filename)\n",
    "            else:\n",
    "                # For each image, predict each pixel's label\n",
    "                prediction_outputs = m.predict(filename)\n",
    "                probs = prediction_outputs['probs'][0]\n",
    "                probs = probs[:, :, 2]  # Take only class '2' (class 0 is the background, class 1 is the document, class 2 is the image)\n",
    "                probs = probs / np.max(probs)  # Normalize to be in [0, 1]\n",
    "\n",
    "                # Binarize the predictions\n",
    "                page_bin = page_make_binary_mask(probs)\n",
    "\n",
    "                # Upscale to have full resolution image (cv2 uses (w,h) and not (h,w) for giving shapes)        \n",
    "                original_shape = prediction_outputs['original_shape']\n",
    "                original_size = tuple(original_shape[::-1])\n",
    "                original_size = (round(original_size[0] / 1024 * int(row['width'])), round(original_size[1] / 1024 * int(row['height'])))\n",
    "                bin_upscaled = cv2.resize(page_bin.astype(np.uint8, copy=False),\n",
    "                                          original_size, interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "                # Find quadrilateral enclosing the page\n",
    "                pred_page_coords = boxes_detection.find_boxes(bin_upscaled.astype(np.uint8, copy=False),\n",
    "                                                              mode='min_rectangle', min_area=0.2, n_max_boxes=1)\n",
    "\n",
    "                # Rescale coordinates\n",
    "                if pred_page_coords is not None:\n",
    "                    row['documentCoordinates'] = format_quad_to_string(pred_page_coords)\n",
    "\n",
    "                    # Store coordinates in data after every prediction\n",
    "                    writeData(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Output as CIDOC-CRM RDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output as a Trig file that can be displayed and edited in the Mirador component of ResearchSpace & Metaphacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "namespaces = \"\"\"\n",
    "@prefix Platform: <http://www.metaphacts.com/ontologies/platform#> .\n",
    "@prefix User: <http://www.metaphacts.com/resource/user/> .\n",
    "@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .\n",
    "@prefix crmdig: <http://www.ics.forth.gr/isl/CRMdig/> .\n",
    "@prefix rso: <http://www.researchspace.org/ontology/> .\n",
    "@prefix prov: <http://www.w3.org/ns/prov#> .\n",
    "@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .\n",
    "@prefix ldp: <http://www.w3.org/ns/ldp#> .\n",
    "@prefix crm: <http://www.cidoc-crm.org/cidoc-crm/>.\n",
    "\"\"\"\n",
    "\n",
    "static = \"\"\"\n",
    "\n",
    "<https://platform.swissartresearch.net/imageRegions> {\n",
    "    <https://resource.swissartresearch.net/type/imageRegion> a crm:E55_Type ;\n",
    "    rdfs:label \"Image Region\" ;\n",
    "    crm:P3_has_note \"A region defining the visual image represented within a digital image. For example, the region denotes the visual item that is reproduced on a document which is photographed.\".\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "regionTemplate = Template('''<$uri/container/context> {\n",
    "  Platform:formContainer ldp:contains <$uri/container> .\n",
    "  \n",
    "  <$uri>\n",
    "    a crmdig:D35_Area, rso:EX_Digital_Image_Region;\n",
    "    crmdig:L49_is_primary_area_of <$iiifImage>;\n",
    "    crm:P33_used_specific_technique <https://github.com/swiss-art-research-net/bso-image-segmentation> ;\n",
    "    rso:boundingBox \"xywh=$x,$y,$w,$h\";\n",
    "    rso:displayLabel \"image\";\n",
    "    rso:viewport \"xywh=0,0,0,0\";\n",
    "    rdf:value \"<svg xmlns='http://www.w3.org/2000/svg'><path xmlns=\\\\\"http://www.w3.org/2000/svg\\\\\" d=\\\\\"M${x0},${y0}l${halfW},0l0,0l${halfW},0l 0,${halfH}l 0,${halfH}l -${halfW},0l -${halfW},0l 0,-${halfH}z\\\\\" data-paper-data=\\\\\"{&quot;defaultStrokeValue&quot;:1,&quot;editStrokeValue&quot;:5,&quot;currentStrokeValue&quot;:1,&quot;rotation&quot;:0,&quot;deleteIcon&quot;:null,&quot;rotationIcon&quot;:null,&quot;group&quot;:null,&quot;editable&quot;:true,&quot;annotation&quot;:null}\\\\\" id=\\\\\"rectangle_e880ad36-1fef-4ce3-835d-716ba7db628a\\\\\" fill-opacity=\\\\\"0\\\\\" fill=\\\\\"#00bfff\\\\\" fill-rule=\\\\\"nonzero\\\\\" stroke=\\\\\"#00bfff\\\\\" stroke-width=\\\\\"4.04992\\\\\" stroke-linecap=\\\\\"butt\\\\\" stroke-linejoin=\\\\\"miter\\\\\" stroke-miterlimit=\\\\\"10\\\\\" stroke-dasharray=\\\\\"\\\\\" stroke-dashoffset=\\\\\"0\\\\\" font-family=\\\\\"none\\\\\" font-weight=\\\\\"none\\\\\" font-size=\\\\\"none\\\\\" text-anchor=\\\\\"none\\\\\" style=\\\\\"mix-blend-mode: normal\\\\\"/></svg>\" .\n",
    "  \n",
    "  <$uri/container>\n",
    "    a ldp:Resource, prov:Entity;\n",
    "    prov:generatedAtTime \"$dateTime\"^^xsd:dateTime;\n",
    "    prov:wasAttributedTo User:admin .\n",
    "}\n",
    "\n",
    "<https://platform.swissartresearch.net/imageRegions> {\n",
    "    <$uri> crm:P2_has_type <https://resource.swissartresearch.net/type/imageRegion> .\n",
    "}\n",
    "\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28103/28103 [00:01<00:00, 15797.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not detect coordinates in 111 images:\n",
      "zbz-990100839710205508\n",
      "zbz-990101416840205508\n",
      "zbz-990101521100205508\n",
      "zbz-990102101150205508\n",
      "zbz-990102101150205508\n",
      "zbz-990102782640205508\n",
      "zbz-990103614330205508\n",
      "zbz-990104876180205508\n",
      "zbz-990105616830205508\n",
      "zbz-990106138050205508\n",
      "zbz-990106138320205508\n",
      "zbz-990106141270205508\n",
      "zbz-990106262790205508\n",
      "zbz-990106263300205508\n",
      "zbz-990107379980205508\n",
      "zbz-990107401490205508\n",
      "zbz-990107405070205508\n",
      "zbz-990107405210205508\n",
      "zbz-990107405300205508\n",
      "zbz-990107454510205508\n",
      "zbz-990107778670205508\n",
      "zbz-990108007420205508\n",
      "zbz-990108007920205508\n",
      "zbz-990108008120205508\n",
      "zbz-990108014810205508\n",
      "zbz-990108015010205508\n",
      "zbz-990108015040205508\n",
      "zbz-990108015120205508\n",
      "zbz-990108015130205508\n",
      "zbz-990108978310205508\n",
      "zbz-990109329750205508\n",
      "zbz-990109597470205508\n",
      "zbz-990110251690205508\n",
      "zbz-990110251830205508\n",
      "zbz-990110251900205508\n",
      "zbz-990110252020205508\n",
      "zbz-990110252050205508\n",
      "zbz-990110607760205508\n",
      "zbz-990110608440205508\n",
      "nb-1001599\n",
      "nb-812808\n",
      "nb-815037\n",
      "nb-815050\n",
      "nb-815054\n",
      "nb-815062\n",
      "nb-815093\n",
      "nb-815097\n",
      "nb-815102\n",
      "nb-815125\n",
      "nb-815670\n",
      "nb-822350\n",
      "nb-838092\n",
      "nb-838152\n",
      "nb-838155\n",
      "nb-838157\n",
      "nb-838160\n",
      "nb-838162\n",
      "nb-838164\n",
      "nb-838166\n",
      "nb-838168\n",
      "nb-838170\n",
      "nb-838172\n",
      "nb-838174\n",
      "nb-838176\n",
      "nb-838178\n",
      "nb-838180\n",
      "nb-838182\n",
      "nb-841831\n",
      "nb-841890\n",
      "nb-861242\n",
      "nb-870419\n",
      "nb-870422\n",
      "nb-991474\n",
      "nb-999914\n",
      "nb-999919\n",
      "nb-999925\n",
      "nb-999926\n",
      "nb-999927\n",
      "nb-999930\n",
      "SFF_B_1609\n",
      "SFF_B_1610\n",
      "SFF_B_1612\n",
      "SFF_B_1617\n",
      "SFF_B_1618\n",
      "SFF_B_1637\n",
      "SFF_D_1001\n",
      "SFF_D_1002\n",
      "SFF_D_1003\n",
      "SFF_D_1005\n",
      "SFF_D_1007\n",
      "SFF_D_1008\n",
      "SFF_D_1010\n",
      "SFF_D_1017\n",
      "SFF_D_1020\n",
      "SFF_D_1034\n",
      "SFF_D_1095\n",
      "SFF_D_1097\n",
      "SFF_D_1098\n",
      "SFF_D_1116\n",
      "SFF_D_1158\n",
      "SFF_D_1159\n",
      "SFF_D_1205\n",
      "SFF_D_1256\n",
      "SFF_D_1301\n",
      "SFF_D_1302\n",
      "SFF_D_1303\n",
      "SFF_D_1475\n",
      "SFF_D_1506\n",
      "SFF_D_1507\n",
      "SFF_D_1509\n",
      "SFF_D_1510\n"
     ]
    }
   ],
   "source": [
    "dateTime = datetime.now().strftime(\"%Y-%m-%dT%H:%M:%Sz\")\n",
    "\n",
    "output = namespaces + static\n",
    "\n",
    "missingDocumentCoordinates = []\n",
    "\n",
    "for row in tqdm(data):\n",
    "    if row['documentCoordinates'] is None:\n",
    "        missingDocumentCoordinates.append(row)\n",
    "        continue\n",
    "        \n",
    "    docCoords = row['documentCoordinates'].split(',')\n",
    "    \n",
    "    if len(docCoords) < 8:\n",
    "        missingDocumentCoordinates.append(row)\n",
    "        continue\n",
    "\n",
    "    xCoords = [int(docCoords[0]), int(docCoords[2]), int(docCoords[4]), int(docCoords[6])]\n",
    "    yCoords = [int(docCoords[1]), int(docCoords[3]), int(docCoords[5]), int(docCoords[7])]\n",
    "    edges = {\n",
    "        \"topLeft\": (min(xCoords), min(yCoords)),\n",
    "        \"topRight\": (max(xCoords), min(yCoords)),\n",
    "        \"bottomRight\": (max(xCoords), max(yCoords)),\n",
    "        \"bottomLeft\": (min(xCoords), max(yCoords))\n",
    "    }\n",
    "    iiifImage = row['image']\n",
    "    identifier = str(uuid.uuid3(uuid.NAMESPACE_DNS, iiifImage))\n",
    "    uri = \"https://resource.swissartresearch.net/digitalobject/\" + identifier\n",
    "    x0 = edges['topLeft'][0]\n",
    "    y0 = edges['topLeft'][1]\n",
    "    x1 = edges['bottomRight'][0]\n",
    "    y1 = edges['bottomRight'][1]\n",
    "    x = x0\n",
    "    y = y0\n",
    "    w = x1 - x0\n",
    "    h = y1 - y0\n",
    "    output += regionTemplate.substitute(\n",
    "        uri=uri,\n",
    "        iiifImage=iiifImage,\n",
    "        x=int(x),\n",
    "        y=int(y),\n",
    "        w=int(w),\n",
    "        h=int(h),\n",
    "        x0=x0,\n",
    "        y0=y0,\n",
    "        halfW=float(w/2),\n",
    "        halfH=float(h/2),\n",
    "        dateTime=dateTime\n",
    "    )\n",
    "\n",
    "# Write summary of missing corodinates\n",
    "if len(missingDocumentCoordinates) > 0:\n",
    "    print(\"Could not detect coordinates in %d images:\" % len(missingDocumentCoordinates))\n",
    "    print('\\n'.join([d['id'] for d in missingDocumentCoordinates]))\n",
    "    \n",
    "filename = path.join(config['trigFile'])\n",
    "with open(filename, 'w') as f:\n",
    "    f.write(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
