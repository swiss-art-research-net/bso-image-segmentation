{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import urllib.request\n",
    "import requests\n",
    "import uuid\n",
    "import time\n",
    "import yaml\n",
    "from os import path\n",
    "from pathlib import Path\n",
    "from string import Template\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "configFile = '../pipeline/config.yml'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open(configFile, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "except:\n",
    "    raise Exception(\"Could not load config file at\", configFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "SPARQL = 0\n",
    "CSV = 1\n",
    "\n",
    "def sparqlResultToDict(results):\n",
    "    rows = []\n",
    "    for result in results[\"results\"][\"bindings\"]:\n",
    "        row = {}\n",
    "        for key in results[\"head\"][\"vars\"]:\n",
    "            if key in result:\n",
    "                row[key] = result[key][\"value\"]\n",
    "            else:\n",
    "                row[key] = None\n",
    "        rows.append(row)\n",
    "    return rows\n",
    "\n",
    "def writeData(data):\n",
    "    try:\n",
    "        with open(config['dataFile'], 'w') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=['id','image','width','height','documentCoordinates'])\n",
    "            writer.writeheader()\n",
    "            for row in data:\n",
    "                if not 'documentCoordinates' in row:\n",
    "                    row['documentCoordinates'] = None\n",
    "                writer.writerow(row)\n",
    "    except:\n",
    "        raise Exception(\"Could not write to\", config['dataFile'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Get input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = False\n",
    "if config['mode'] == \"SPARQL\":\n",
    "    mode = SPARQL\n",
    "elif config['mode'] == \"CSV\":\n",
    "    mode  = CSV\n",
    "else:\n",
    "    raise Exception(\"mode not specified or invalid (should be SPARQL or CSV)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read data from input file, if present. This is being done for both CSV and SPARQL mode as the SPARQL results will be cashed in the CSV file and updated when data is changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputData = []\n",
    "try:\n",
    "    with open(config['dataFile'], 'r') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            inputData.append({\n",
    "                \"id\": row['id'],\n",
    "                \"image\": row['image'],\n",
    "                \"width\": row['width'],\n",
    "                \"height\": row['height'],\n",
    "                \"documentCoordinates\": row['documentCoordinates'] if 'documentCoordinates' in row else None\n",
    "            })\n",
    "except:\n",
    "    print(\"No prior input file found\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If in SPARQL mode, get data from SPARQL endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mode == SPARQL:\n",
    "    if not config['endpoint'] or not config['query']:\n",
    "        raise Exception(\"incomplete configuration for SPARQL mode\")\n",
    "        \n",
    "    sparql = SPARQLWrapper(config['endpoint'], returnFormat=JSON)\n",
    "    sparql.setQuery(config['query'])\n",
    "    try:\n",
    "        ret = sparql.query().convert()\n",
    "    except:\n",
    "        raise Exception(\"Could not execute query against endpoint\", config['endpoint'])\n",
    "    queriedData = sparqlResultToDict(ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If in SPARQL mode, merge queried data with data stored in CSV file.\n",
    "- add entries that exist in SPARQL result, but not in the CSV file\n",
    "- add width/height information when it is only available in either the CSV file or the SPARQL output (prioritising the SPARQL data)\n",
    "Store merged data in CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = inputData\n",
    "\n",
    "if mode == SPARQL:\n",
    "    inputDataHash = {}\n",
    "    queriedDataHash = {}\n",
    "\n",
    "    for row in inputData:\n",
    "        inputDataHash[row['id']] = row\n",
    "    for row in queriedData:\n",
    "        queriedDataHash[row['id']] = row\n",
    "\n",
    "    idsInInputData = [d['id'] for d in inputData]\n",
    "    for row in queriedData:\n",
    "        if row['id'] not in idsInInputData:\n",
    "            data.append(row)\n",
    "\n",
    "    for row in data:\n",
    "        if not row['width']:\n",
    "            if row['id'] in queriedDataHash and queriedDataHash[row['id']]['width']:\n",
    "                row['width'] = queriedDataHash[row['id']]['width']\n",
    "            elif row['id'] in inputDataHash and inputDataHash[row['id']]['width']:\n",
    "                row['width'] = inputDataHash[row['id']]['width']\n",
    "        if not row['height']:\n",
    "            if row['id'] in queriedDataHash and queriedDataHash[row['id']]['height']:\n",
    "                row['height'] = queriedDataHash[row['id']]['height']\n",
    "            elif row['id'] in inputDataHash and inputDataHash[row['id']]['width']:\n",
    "                row['height'] = inputDataHash[row['id']]['height']\n",
    "    \n",
    "    writeData(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Get (missing) image sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the original image size is not specified, call the IIIF Image API to read the size from the JSON rsponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28103/28103 [00:00<00:00, 270565.78it/s]\n"
     ]
    }
   ],
   "source": [
    "for row in tqdm(data):\n",
    "    if not row['width'] or not row['height']:\n",
    "        uri = row['image'] + '/info.json'\n",
    "        try:\n",
    "            with urllib.request.urlopen(uri) as url:\n",
    "                manifestData = json.loads(url.read().decode())\n",
    "                \n",
    "        except:\n",
    "            print(\"Could not open\", uri)\n",
    "            next\n",
    "        row['width'] = manifestData['width']\n",
    "        row['height'] = manifestData['height']\n",
    "        writeData(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write data to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "writeData(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Download images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the images that do not yet exist in the image folder. The images will be downloaded resized to a width of 1024 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    Path(config['imageDirectory']).mkdir(parents=True, exist_ok=True)\n",
    "except:\n",
    "    raise Exception(\"Could not add/access folder\", config['imageDirectory'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Detect Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  3.35it/s]\n"
     ]
    }
   ],
   "source": [
    "extension = 50\n",
    "\n",
    "def getContours(file, extension):\n",
    "    # Load image\n",
    "    image = cv2.imread(file)\n",
    "    image = image[:,:,::-1]\n",
    "\n",
    "    # Extend image border\n",
    "    extendedImage = cv2.copyMakeBorder(image,extension,extension,extension,extension,cv2.BORDER_REPLICATE)\n",
    "\n",
    "    # Convert to grayscale and blur\n",
    "    gray = cv2.cvtColor(extendedImage.copy(), cv2.COLOR_RGB2GRAY)\n",
    "    gray = cv2.blur(gray, (5, 5))\n",
    "\n",
    "    # Binarise\n",
    "    # Check brighness of upper left corner\n",
    "    # if bright use cv2.THRESH_BINARY_INV\n",
    "    # if not use cv2.THRESH_BINARY\n",
    "    padding = 5\n",
    "    if gray[extension+padding][image.shape[1]-padding] > 127:\n",
    "        thresholdMethod = cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU\n",
    "    else:\n",
    "        thresholdMethod = cv2.THRESH_BINARY+cv2.THRESH_OTSU\n",
    "        \n",
    "    ret, thresh = cv2.threshold(gray,0,255,thresholdMethod)\n",
    "    \n",
    "    # Extend (Dilate)\n",
    "    #kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3,3))\n",
    "    #dilate = cv2.dilate(thresh, kernel, iterations=3)\n",
    "\n",
    "    # Detect contours\n",
    "    contours, hierarchy = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    return contours\n",
    "\n",
    "def getBoundingBoxOfImage(contours, extension):\n",
    "    # We want to use the biggest contour detected.\n",
    "    # However, in some cases this might be the colour bar,\n",
    "    # so we check for the two largest contours and take the one\n",
    "    # where the image ratio is close to square. The colour bar\n",
    "    # has an image ratio (w/h or h/w) of about 3\n",
    "    \n",
    "    areas = [cv2.contourArea(c) for c in contours]\n",
    "    \n",
    "    indicesOfTwoLargestContours = [areas.index(x) for x in sorted(areas, reverse=True)[:2]]\n",
    "    \n",
    "    ratios = []\n",
    "    for i in indicesOfTwoLargestContours:\n",
    "        x,y,w,h = cv2.boundingRect(contours[i])\n",
    "        ratios.append(max(w,h)/min(w,h))\n",
    "    minRatioIndex = np.argmin(ratios)\n",
    "\n",
    "    x,y,w,h = cv2.boundingRect(contours[indicesOfTwoLargestContours[minRatioIndex]])\n",
    "    \n",
    "    # Remove frame\n",
    "    x = x - extension if x > extension else 0\n",
    "    y = y - extension if y > extension else 0\n",
    "    return x,y,w,h\n",
    "\n",
    "for row in tqdm(data):\n",
    "    if not row['documentCoordinates'] or len(row['documentCoordinates']) == 0:\n",
    "        \n",
    "        filename = path.join(config['imageDirectory'], row['id'] + '.jpg')\n",
    "        if isfile(filename):\n",
    "            contours = getContours(filename, extension)\n",
    "            x,y,w,h = getBoundingBoxOfImage(contours, extension)\n",
    "\n",
    "            # Upscale to original size\n",
    "            originalWidth = int(row['width'])\n",
    "            originalHeight = int(row['height'])\n",
    "            row['documentCoordinates'] = \"%d,%d,%d,%d\" % (int( x / 1024 * originalWidth),\n",
    "                                                          int( y / 1024 * originalHeight),\n",
    "                                                          int( w / 1024 * originalWidth),\n",
    "                                                          int( h / 1024 * originalHeight))\n",
    "\n",
    "            # Store coordinates in data after every prediction\n",
    "            writeData(data)\n",
    "        else:\n",
    "            print(\"Could not open\", filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Output as CIDOC-CRM RDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output as a Trig file that can be displayed and edited in the Mirador component of ResearchSpace & Metaphacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "namespaces = \"\"\"\n",
    "PREFIX Platform: <http://www.metaphacts.com/ontologies/platform#> \n",
    "PREFIX User: <http://www.metaphacts.com/resource/user/> \n",
    "PREFIX xsd: <http://www.w3.org/2001/XMLSchema#> \n",
    "PREFIX crmdig: <http://www.ics.forth.gr/isl/CRMdig/> \n",
    "PREFIX rso: <http://www.researchspace.org/ontology/> \n",
    "PREFIX prov: <http://www.w3.org/ns/prov#> \n",
    "PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> \n",
    "PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "PREFIX ldp: <http://www.w3.org/ns/ldp#> \n",
    "PREFIX crm: <http://www.cidoc-crm.org/cidoc-crm/>\n",
    "\"\"\"\n",
    "\n",
    "static = \"\"\"\n",
    "\n",
    "<https://platform.swissartresearch.net/imageRegions> {\n",
    "    <https://resource.swissartresearch.net/type/imageRegion> a crm:E55_Type ;\n",
    "    rdfs:label \"Image Region\" ;\n",
    "    crm:P3_has_note \"A region defining the visual image represented within a digital image. For example, the region denotes the visual item that is reproduced on a document which is photographed.\".\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "regionTemplate = Template('''<$uri/container/context> {\n",
    "  Platform:formContainer ldp:contains <$uri/container> .\n",
    "  \n",
    "  <$uri>\n",
    "    a crmdig:D35_Area, rso:EX_Digital_Image_Region;\n",
    "    crmdig:L49_is_primary_area_of <$iiifImage>;\n",
    "    crm:P33_used_specific_technique <https://github.com/swiss-art-research-net/bso-image-segmentation> ;\n",
    "    rso:boundingBox \"xywh=$x,$y,$w,$h\";\n",
    "    rso:displayLabel \"image\";\n",
    "    rso:viewport \"xywh=0,0,0,0\";\n",
    "    rdf:value \"<svg xmlns='http://www.w3.org/2000/svg'><path xmlns=\\\\\"http://www.w3.org/2000/svg\\\\\" d=\\\\\"M${x0},${y0}l${halfW},0l0,0l${halfW},0l 0,${halfH}l 0,${halfH}l -${halfW},0l -${halfW},0l 0,-${halfH}z\\\\\" data-paper-data=\\\\\"{&quot;defaultStrokeValue&quot;:1,&quot;editStrokeValue&quot;:5,&quot;currentStrokeValue&quot;:1,&quot;rotation&quot;:0,&quot;deleteIcon&quot;:null,&quot;rotationIcon&quot;:null,&quot;group&quot;:null,&quot;editable&quot;:true,&quot;annotation&quot;:null}\\\\\" id=\\\\\"rectangle_e880ad36-1fef-4ce3-835d-716ba7db628a\\\\\" fill-opacity=\\\\\"0\\\\\" fill=\\\\\"#00bfff\\\\\" fill-rule=\\\\\"nonzero\\\\\" stroke=\\\\\"#00bfff\\\\\" stroke-width=\\\\\"4.04992\\\\\" stroke-linecap=\\\\\"butt\\\\\" stroke-linejoin=\\\\\"miter\\\\\" stroke-miterlimit=\\\\\"10\\\\\" stroke-dasharray=\\\\\"\\\\\" stroke-dashoffset=\\\\\"0\\\\\" font-family=\\\\\"none\\\\\" font-weight=\\\\\"none\\\\\" font-size=\\\\\"none\\\\\" text-anchor=\\\\\"none\\\\\" style=\\\\\"mix-blend-mode: normal\\\\\"/></svg>\" .\n",
    "  \n",
    "  <$uri/container>\n",
    "    a ldp:Resource, prov:Entity;\n",
    "    prov:generatedAtTime \"$dateTime\"^^xsd:dateTime;\n",
    "    prov:wasAttributedTo User:admin .\n",
    "}\n",
    "\n",
    "<https://platform.swissartresearch.net/imageRegions> {\n",
    "    <$uri> crm:P2_has_type <https://resource.swissartresearch.net/type/imageRegion> .\n",
    "}\n",
    "\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 10/28103 [00:00<00:04, 5789.24it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'bool' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-76ccb6f6e8f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mdocCoords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'documentCoordinates'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocCoords\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'bool' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "dateTime = datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S+00:00z\")\n",
    "\n",
    "output = namespaces + static\n",
    "\n",
    "missingDocumentCoordinates = []\n",
    "\n",
    "for row in tqdm(data):\n",
    "    if row['documentCoordinates'] is None:\n",
    "        missingDocumentCoordinates.append(row)\n",
    "        continue\n",
    "        \n",
    "    docCoords = row['documentCoordinates'].split(',')\n",
    "    \n",
    "    if len(docCoords) < 4:\n",
    "        missingDocumentCoordinates.append(row)\n",
    "        continue\n",
    "\n",
    "    x = int(docCoords[0])\n",
    "    y = int(docCoords[1])\n",
    "    w = int(docCoords[2])\n",
    "    h = int(docCoords[3])\n",
    "\n",
    "    edges = {\n",
    "        \"topLeft\": (x, y),\n",
    "        \"topRight\": (x + w, y),\n",
    "        \"bottomRight\": (x + w, y + h),\n",
    "        \"bottomLeft\": (x, y + h)\n",
    "    }\n",
    "    iiifImage = row['image']\n",
    "    identifier = str(uuid.uuid3(uuid.NAMESPACE_DNS, iiifImage))\n",
    "    uri = \"https://resource.swissartresearch.net/digitalobject/\" + identifier\n",
    "    x0 = edges['topLeft'][0]\n",
    "    y0 = edges['topLeft'][1]\n",
    "    x1 = edges['bottomRight'][0]\n",
    "    y1 = edges['bottomRight'][1]\n",
    "    x = x0\n",
    "    y = y0\n",
    "    w = x1 - x0\n",
    "    h = y1 - y0\n",
    "    output += regionTemplate.substitute(\n",
    "        uri=uri,\n",
    "        iiifImage=iiifImage,\n",
    "        x=int(x),\n",
    "        y=int(y),\n",
    "        w=int(w),\n",
    "        h=int(h),\n",
    "        x0=x0,\n",
    "        y0=y0,\n",
    "        halfW=float(w/2),\n",
    "        halfH=float(h/2),\n",
    "        dateTime=dateTime\n",
    "    )\n",
    "\n",
    "# Write summary of missing corodinates\n",
    "if len(missingDocumentCoordinates) > 0:\n",
    "    print(\"Could not detect coordinates in %d images:\" % len(missingDocumentCoordinates))\n",
    "    print('\\n'.join([d['id'] for d in missingDocumentCoordinates]))\n",
    "    \n",
    "filename = path.join(config['trigFile'])\n",
    "with open(filename, 'w') as f:\n",
    "    f.write(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
