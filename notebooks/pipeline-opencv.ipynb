{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import csv\n",
    "import json\n",
    "import urllib.request\n",
    "import requests\n",
    "import uuid\n",
    "import time\n",
    "import yaml\n",
    "from os import path\n",
    "from pathlib import Path\n",
    "from string import Template\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "configFile = '../pipeline/config.yml'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open(configFile, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "except:\n",
    "    raise Exception(\"Could not load config file at\", configFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "SPARQL = 0\n",
    "CSV = 1\n",
    "\n",
    "def sparqlResultToDict(results):\n",
    "    rows = []\n",
    "    for result in results[\"results\"][\"bindings\"]:\n",
    "        row = {}\n",
    "        for key in results[\"head\"][\"vars\"]:\n",
    "            if key in result:\n",
    "                row[key] = result[key][\"value\"]\n",
    "            else:\n",
    "                row[key] = None\n",
    "        rows.append(row)\n",
    "    return rows\n",
    "\n",
    "def writeData(data):\n",
    "    try:\n",
    "        with open(config['dataFile'], 'w') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=['id','image','width','height','documentCoordinates'])\n",
    "            writer.writeheader()\n",
    "            for row in data:\n",
    "                if not 'documentCoordinates' in row:\n",
    "                    row['documentCoordinates'] = None\n",
    "                writer.writerow(row)\n",
    "    except:\n",
    "        raise Exception(\"Could not write to\", config['dataFile'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Get input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = False\n",
    "if config['mode'] == \"SPARQL\":\n",
    "    mode = SPARQL\n",
    "elif config['mode'] == \"CSV\":\n",
    "    mode  = CSV\n",
    "else:\n",
    "    raise Exception(\"mode not specified or invalid (should be SPARQL or CSV)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read data from input file, if present. This is being done for both CSV and SPARQL mode as the SPARQL results will be cashed in the CSV file and updated when data is changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputData = []\n",
    "try:\n",
    "    with open(config['dataFile'], 'r') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            inputData.append({\n",
    "                \"id\": row['id'],\n",
    "                \"image\": row['image'],\n",
    "                \"width\": row['width'],\n",
    "                \"height\": row['height'],\n",
    "                \"documentCoordinates\": row['documentCoordinates'] if 'documentCoordinates' in row else None\n",
    "            })\n",
    "except:\n",
    "    print(\"No prior input file found\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If in SPARQL mode, get data from SPARQL endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mode == SPARQL:\n",
    "    if not config['endpoint'] or not config['query']:\n",
    "        raise Exception(\"incomplete configuration for SPARQL mode\")\n",
    "        \n",
    "    sparql = SPARQLWrapper(config['endpoint'], returnFormat=JSON)\n",
    "    sparql.setQuery(config['query'])\n",
    "    try:\n",
    "        ret = sparql.query().convert()\n",
    "    except:\n",
    "        raise Exception(\"Could not execute query against endpoint\", config['endpoint'])\n",
    "    queriedData = sparqlResultToDict(ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If in SPARQL mode, merge queried data with data stored in CSV file.\n",
    "- add entries that exist in SPARQL result, but not in the CSV file\n",
    "- add width/height information when it is only available in either the CSV file or the SPARQL output (prioritising the SPARQL data)\n",
    "Store merged data in CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = inputData\n",
    "\n",
    "if mode == SPARQL:\n",
    "    inputDataHash = {}\n",
    "    queriedDataHash = {}\n",
    "\n",
    "    for row in inputData:\n",
    "        inputDataHash[row['id']] = row\n",
    "    for row in queriedData:\n",
    "        queriedDataHash[row['id']] = row\n",
    "\n",
    "    idsInInputData = [d['id'] for d in inputData]\n",
    "    for row in queriedData:\n",
    "        if row['id'] not in idsInInputData:\n",
    "            data.append(row)\n",
    "\n",
    "    for row in data:\n",
    "        if not row['width']:\n",
    "            if row['id'] in queriedDataHash and queriedDataHash[row['id']]['width']:\n",
    "                row['width'] = queriedDataHash[row['id']]['width']\n",
    "            elif row['id'] in inputDataHash and inputDataHash[row['id']]['width']:\n",
    "                row['width'] = inputDataHash[row['id']]['width']\n",
    "        if not row['height']:\n",
    "            if row['id'] in queriedDataHash and queriedDataHash[row['id']]['height']:\n",
    "                row['height'] = queriedDataHash[row['id']]['height']\n",
    "            elif row['id'] in inputDataHash and inputDataHash[row['id']]['width']:\n",
    "                row['height'] = inputDataHash[row['id']]['height']\n",
    "    \n",
    "    writeData(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Get (missing) image sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the original image size is not specified, call the IIIF Image API to read the size from the JSON rsponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28103/28103 [00:00<00:00, 267240.40it/s]\n"
     ]
    }
   ],
   "source": [
    "for row in tqdm(data):\n",
    "    if not row['width'] or not row['height']:\n",
    "        uri = row['image'] + '/info.json'\n",
    "        try:\n",
    "            with urllib.request.urlopen(uri) as url:\n",
    "                manifestData = json.loads(url.read().decode())\n",
    "                \n",
    "        except:\n",
    "            print(\"Could not open\", uri)\n",
    "            next\n",
    "        row['width'] = manifestData['width']\n",
    "        row['height'] = manifestData['height']\n",
    "        writeData(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write data to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "writeData(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Download images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the images that do not yet exist in the image folder. The images will be downloaded resized to a width of 1024 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    Path(config['imageDirectory']).mkdir(parents=True, exist_ok=True)\n",
    "except:\n",
    "    raise Exception(\"Could not add/access folder\", config['imageDirectory'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Detect Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import inspect\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parentdir = os.path.dirname(currentdir)\n",
    "sys.path.insert(0, parentdir) \n",
    "\n",
    "from openCV.BSOImageCropping import BSOImageCropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28103/28103 [00:00<00:00, 222727.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not open ../data/images/nb-812808.jpg\n",
      "Could not open ../data/images/nb-815037.jpg\n",
      "Could not open ../data/images/nb-815050.jpg\n",
      "Could not open ../data/images/nb-815054.jpg\n",
      "Could not open ../data/images/nb-815062.jpg\n",
      "Could not open ../data/images/nb-815093.jpg\n",
      "Could not open ../data/images/nb-815097.jpg\n",
      "Could not open ../data/images/nb-815102.jpg\n",
      "Could not open ../data/images/nb-815125.jpg\n",
      "Could not open ../data/images/nb-815670.jpg\n",
      "Could not open ../data/images/nb-822350.jpg\n",
      "Could not open ../data/images/nb-838092.jpg\n",
      "Could not open ../data/images/nb-838152.jpg\n",
      "Could not open ../data/images/nb-838155.jpg\n",
      "Could not open ../data/images/nb-838157.jpg\n",
      "Could not open ../data/images/nb-838160.jpg\n",
      "Could not open ../data/images/nb-838162.jpg\n",
      "Could not open ../data/images/nb-838164.jpg\n",
      "Could not open ../data/images/nb-838166.jpg\n",
      "Could not open ../data/images/nb-838168.jpg\n",
      "Could not open ../data/images/nb-838170.jpg\n",
      "Could not open ../data/images/nb-838172.jpg\n",
      "Could not open ../data/images/nb-838174.jpg\n",
      "Could not open ../data/images/nb-838176.jpg\n",
      "Could not open ../data/images/nb-838178.jpg\n",
      "Could not open ../data/images/nb-838180.jpg\n",
      "Could not open ../data/images/nb-838182.jpg\n",
      "Could not open ../data/images/nb-841831.jpg\n",
      "Could not open ../data/images/nb-841890.jpg\n",
      "Could not open ../data/images/nb-861242.jpg\n",
      "Could not open ../data/images/nb-870419.jpg\n",
      "Could not open ../data/images/nb-870422.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "detector = BSOImageCropping()\n",
    "    \n",
    "for row in tqdm(data):\n",
    "    if not row['documentCoordinates'] or len(row['documentCoordinates']) == 0:\n",
    "        \n",
    "        filename = path.join(config['imageDirectory'], row['id'] + '.jpg')\n",
    "        if path.isfile(filename):\n",
    "            image = cv2.imread(filename)\n",
    "            image = image[:,:,::-1]\n",
    "            if not 'zbz' in filename:\n",
    "                x,y,w,h = detector.cropImage(image, selectMethod=BSOImageCropping.SELECT_LARGEST)\n",
    "            else:\n",
    "                x,y,w,h = detector.cropImage(image, selectMethod=BSOImageCropping.SELECT_SQUAREST)\n",
    "\n",
    "            # Upscale to original size\n",
    "            scaleFactor = int(row['width'])/image.shape[1]\n",
    "            row['documentCoordinates'] = \"%d,%d,%d,%d\" % (int( x * scaleFactor),\n",
    "                                                          int( y * scaleFactor),\n",
    "                                                          int( w * scaleFactor),\n",
    "                                                          int( h * scaleFactor))\n",
    "\n",
    "            # Store coordinates in data after every prediction\n",
    "            writeData(data)\n",
    "        else:\n",
    "            print(\"Could not open\", filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Validate image regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do some automatised quality control. Check if the image regions detected exceed a certain size. If not, try again with a different strategy. If they are still too small, discard the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "minSizeRatio = 0.1 # Detected region should cover at least x% of width or height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "toCheck = []\n",
    "for row in data:\n",
    "    if row['documentCoordinates']:\n",
    "        boundingBox = row['documentCoordinates'].split(\",\")\n",
    "        boxWidth = int(boundingBox[2])\n",
    "        boxHeight = int(boundingBox[3])+1      \n",
    "        imageWidth = int(row['width'])\n",
    "        imageHeight = int(row['height'])\n",
    "        if boxWidth / imageWidth < minSizeRatio or boxHeight / imageHeight < minSizeRatio:\n",
    "            toCheck.append(row)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 471 image crops that might be too small\n"
     ]
    }
   ],
   "source": [
    "print(\"Found %d image crops that might be too small\" % len(toCheck))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 471/471 [00:14<00:00, 31.93it/s]\n"
     ]
    }
   ],
   "source": [
    "recalculated = []\n",
    "discarded = []\n",
    "\n",
    "for row in tqdm(toCheck):    \n",
    "    filename = path.join(config['imageDirectory'], row['id'] + '.jpg')\n",
    "    if path.isfile(filename):\n",
    "        image = cv2.imread(filename)\n",
    "        image = image[:,:,::-1]\n",
    "        # Try again with largest strategy\n",
    "        x,y,w,h = detector.cropImage(image, selectMethod=BSOImageCropping.SELECT_LARGEST)\n",
    "        if w / image.shape[1] > minSizeRatio and h / image.shape[2] > minSizeRatio:\n",
    "            scaleFactor = int(row['width'])/image.shape[1]\n",
    "            row['documentCoordinates'] = \"%d,%d,%d,%d\" % (int( x * scaleFactor),\n",
    "                                                          int( y * scaleFactor),\n",
    "                                                          int( w * scaleFactor),\n",
    "                                                          int( h * scaleFactor))\n",
    "            recalculated.append(row)\n",
    "        else:\n",
    "            row['documentCoordinates'] = None\n",
    "            discarded.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recalculated 456 image regions\n",
      "Discarded 15 image regions\n"
     ]
    }
   ],
   "source": [
    "print(\"Recalculated %d image regions\" % len (recalculated))\n",
    "print(\"Discarded %d image regions\" % len (discarded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Output as CIDOC-CRM RDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output as a Trig file that can be displayed and edited in the Mirador component of ResearchSpace & Metaphacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "namespaces = \"\"\"\n",
    "PREFIX Platform: <http://www.metaphacts.com/ontologies/platform#> \n",
    "PREFIX User: <http://www.metaphacts.com/resource/user/> \n",
    "PREFIX xsd: <http://www.w3.org/2001/XMLSchema#> \n",
    "PREFIX crmdig: <http://www.ics.forth.gr/isl/CRMdig/> \n",
    "PREFIX rso: <http://www.researchspace.org/ontology/> \n",
    "PREFIX prov: <http://www.w3.org/ns/prov#> \n",
    "PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> \n",
    "PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "PREFIX ldp: <http://www.w3.org/ns/ldp#> \n",
    "PREFIX crm: <http://www.cidoc-crm.org/cidoc-crm/>\n",
    "\"\"\"\n",
    "\n",
    "static = \"\"\"\n",
    "\n",
    "<https://platform.swissartresearch.net/imageRegions> {\n",
    "    <https://resource.swissartresearch.net/type/imageRegion> a crm:E55_Type ;\n",
    "    rdfs:label \"Image Region\" ;\n",
    "    crm:P3_has_note \"A region defining the visual image represented within a digital image. For example, the region denotes the visual item that is reproduced on a document which is photographed.\".\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "regionTemplate = Template('''<$uri/container/context> {\n",
    "  Platform:formContainer ldp:contains <$uri/container> .\n",
    "  \n",
    "  <$uri>\n",
    "    a crmdig:D35_Area, rso:EX_Digital_Image_Region;\n",
    "    crmdig:L49_is_primary_area_of <$iiifImage>;\n",
    "    crm:P33_used_specific_technique <https://github.com/swiss-art-research-net/bso-image-segmentation> ;\n",
    "    rso:boundingBox \"xywh=$x,$y,$w,$h\";\n",
    "    rso:displayLabel \"image\";\n",
    "    rso:viewport \"xywh=0,0,0,0\";\n",
    "    rdf:value \"<svg xmlns='http://www.w3.org/2000/svg'><path xmlns=\\\\\"http://www.w3.org/2000/svg\\\\\" d=\\\\\"M${x0},${y0}l${halfW},0l0,0l${halfW},0l 0,${halfH}l 0,${halfH}l -${halfW},0l -${halfW},0l 0,-${halfH}z\\\\\" data-paper-data=\\\\\"{&quot;defaultStrokeValue&quot;:1,&quot;editStrokeValue&quot;:5,&quot;currentStrokeValue&quot;:1,&quot;rotation&quot;:0,&quot;deleteIcon&quot;:null,&quot;rotationIcon&quot;:null,&quot;group&quot;:null,&quot;editable&quot;:true,&quot;annotation&quot;:null}\\\\\" id=\\\\\"rectangle_e880ad36-1fef-4ce3-835d-716ba7db628a\\\\\" fill-opacity=\\\\\"0\\\\\" fill=\\\\\"#00bfff\\\\\" fill-rule=\\\\\"nonzero\\\\\" stroke=\\\\\"#00bfff\\\\\" stroke-width=\\\\\"4.04992\\\\\" stroke-linecap=\\\\\"butt\\\\\" stroke-linejoin=\\\\\"miter\\\\\" stroke-miterlimit=\\\\\"10\\\\\" stroke-dasharray=\\\\\"\\\\\" stroke-dashoffset=\\\\\"0\\\\\" font-family=\\\\\"none\\\\\" font-weight=\\\\\"none\\\\\" font-size=\\\\\"none\\\\\" text-anchor=\\\\\"none\\\\\" style=\\\\\"mix-blend-mode: normal\\\\\"/></svg>\" .\n",
    "  \n",
    "  <$uri/container>\n",
    "    a ldp:Resource, prov:Entity;\n",
    "    prov:generatedAtTime \"$dateTime\"^^xsd:dateTime;\n",
    "    prov:wasAttributedTo User:admin .\n",
    "}\n",
    "\n",
    "<https://platform.swissartresearch.net/imageRegions> {\n",
    "    <$uri> crm:P2_has_type <https://resource.swissartresearch.net/type/imageRegion> .\n",
    "}\n",
    "\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28103/28103 [00:01<00:00, 20612.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not detect coordinates in 47 images:\n",
      "zbz-990099678450205508\n",
      "zbz-990100190680205508\n",
      "zbz-990100842470205508\n",
      "zbz-990101003860205508\n",
      "zbz-990101237540205508\n",
      "zbz-990101242490205508\n",
      "zbz-990101296780205508\n",
      "zbz-990101594940205508\n",
      "zbz-990101633300205508\n",
      "zbz-990101633600205508\n",
      "zbz-990101910140205508\n",
      "zbz-990101911400205508\n",
      "nb-1050431\n",
      "nb-812808\n",
      "nb-815037\n",
      "nb-815050\n",
      "nb-815054\n",
      "nb-815062\n",
      "nb-815093\n",
      "nb-815097\n",
      "nb-815102\n",
      "nb-815125\n",
      "nb-815670\n",
      "nb-822350\n",
      "nb-838092\n",
      "nb-838152\n",
      "nb-838155\n",
      "nb-838157\n",
      "nb-838160\n",
      "nb-838162\n",
      "nb-838164\n",
      "nb-838166\n",
      "nb-838168\n",
      "nb-838170\n",
      "nb-838172\n",
      "nb-838174\n",
      "nb-838176\n",
      "nb-838178\n",
      "nb-838180\n",
      "nb-838182\n",
      "nb-841831\n",
      "nb-841890\n",
      "nb-861242\n",
      "nb-870419\n",
      "nb-870422\n",
      "nb-894459\n",
      "nb-919292\n"
     ]
    }
   ],
   "source": [
    "dateTime = datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S+00:00z\")\n",
    "\n",
    "output = namespaces + static\n",
    "\n",
    "missingDocumentCoordinates = []\n",
    "\n",
    "for row in tqdm(data):\n",
    "    if row['documentCoordinates'] is None:\n",
    "        missingDocumentCoordinates.append(row)\n",
    "        continue\n",
    "        \n",
    "    docCoords = row['documentCoordinates'].split(',')\n",
    "    \n",
    "    if len(docCoords) < 4:\n",
    "        missingDocumentCoordinates.append(row)\n",
    "        continue\n",
    "\n",
    "    x = int(docCoords[0])\n",
    "    y = int(docCoords[1])\n",
    "    w = int(docCoords[2])\n",
    "    h = int(docCoords[3])\n",
    "\n",
    "    edges = {\n",
    "        \"topLeft\": (x, y),\n",
    "        \"topRight\": (x + w, y),\n",
    "        \"bottomRight\": (x + w, y + h),\n",
    "        \"bottomLeft\": (x, y + h)\n",
    "    }\n",
    "    iiifImage = row['image']\n",
    "    identifier = str(uuid.uuid3(uuid.NAMESPACE_DNS, iiifImage))\n",
    "    uri = \"https://resource.swissartresearch.net/digitalobject/\" + identifier\n",
    "    x0 = edges['topLeft'][0]\n",
    "    y0 = edges['topLeft'][1]\n",
    "    x1 = edges['bottomRight'][0]\n",
    "    y1 = edges['bottomRight'][1]\n",
    "    x = x0\n",
    "    y = y0\n",
    "    w = x1 - x0\n",
    "    h = y1 - y0\n",
    "    output += regionTemplate.substitute(\n",
    "        uri=uri,\n",
    "        iiifImage=iiifImage,\n",
    "        x=int(x),\n",
    "        y=int(y),\n",
    "        w=int(w),\n",
    "        h=int(h),\n",
    "        x0=x0,\n",
    "        y0=y0,\n",
    "        halfW=float(w/2),\n",
    "        halfH=float(h/2),\n",
    "        dateTime=dateTime\n",
    "    )\n",
    "\n",
    "# Write summary of missing corodinates\n",
    "if len(missingDocumentCoordinates) > 0:\n",
    "    print(\"Could not detect coordinates in %d images:\" % len(missingDocumentCoordinates))\n",
    "    print('\\n'.join([d['id'] for d in missingDocumentCoordinates]))\n",
    "    \n",
    "filename = path.join(config['trigFile'])\n",
    "with open(filename, 'w') as f:\n",
    "    f.write(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
