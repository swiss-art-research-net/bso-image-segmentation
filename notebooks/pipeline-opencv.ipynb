{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import urllib.request\n",
    "import requests\n",
    "import uuid\n",
    "import time\n",
    "import yaml\n",
    "from os import path\n",
    "from pathlib import Path\n",
    "from string import Template\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "configFile = '../pipeline/config.yml'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open(configFile, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "except:\n",
    "    raise Exception(\"Could not load config file at\", configFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "SPARQL = 0\n",
    "CSV = 1\n",
    "\n",
    "def sparqlResultToDict(results):\n",
    "    rows = []\n",
    "    for result in results[\"results\"][\"bindings\"]:\n",
    "        row = {}\n",
    "        for key in results[\"head\"][\"vars\"]:\n",
    "            if key in result:\n",
    "                row[key] = result[key][\"value\"]\n",
    "            else:\n",
    "                row[key] = None\n",
    "        rows.append(row)\n",
    "    return rows\n",
    "\n",
    "def writeData(data):\n",
    "    try:\n",
    "        with open(config['dataFile'], 'w') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=['id','image','width','height','documentCoordinates'])\n",
    "            writer.writeheader()\n",
    "            for row in data:\n",
    "                if not 'documentCoordinates' in row:\n",
    "                    row['documentCoordinates'] = None\n",
    "                writer.writerow(row)\n",
    "    except:\n",
    "        raise Exception(\"Could not write to\", config['dataFile'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Get input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = False\n",
    "if config['mode'] == \"SPARQL\":\n",
    "    mode = SPARQL\n",
    "elif config['mode'] == \"CSV\":\n",
    "    mode  = CSV\n",
    "else:\n",
    "    raise Exception(\"mode not specified or invalid (should be SPARQL or CSV)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read data from input file, if present. This is being done for both CSV and SPARQL mode as the SPARQL results will be cashed in the CSV file and updated when data is changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputData = []\n",
    "try:\n",
    "    with open(config['dataFile'], 'r') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            inputData.append({\n",
    "                \"id\": row['id'],\n",
    "                \"image\": row['image'],\n",
    "                \"width\": row['width'],\n",
    "                \"height\": row['height'],\n",
    "                \"documentCoordinates\": row['documentCoordinates'] if 'documentCoordinates' in row else None\n",
    "            })\n",
    "except:\n",
    "    print(\"No prior input file found\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If in SPARQL mode, get data from SPARQL endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mode == SPARQL:\n",
    "    if not config['endpoint'] or not config['query']:\n",
    "        raise Exception(\"incomplete configuration for SPARQL mode\")\n",
    "        \n",
    "    sparql = SPARQLWrapper(config['endpoint'], returnFormat=JSON)\n",
    "    sparql.setQuery(config['query'])\n",
    "    try:\n",
    "        ret = sparql.query().convert()\n",
    "    except:\n",
    "        raise Exception(\"Could not execute query against endpoint\", config['endpoint'])\n",
    "    queriedData = sparqlResultToDict(ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If in SPARQL mode, merge queried data with data stored in CSV file.\n",
    "- add entries that exist in SPARQL result, but not in the CSV file\n",
    "- add width/height information when it is only available in either the CSV file or the SPARQL output (prioritising the SPARQL data)\n",
    "Store merged data in CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = inputData\n",
    "\n",
    "if mode == SPARQL:\n",
    "    inputDataHash = {}\n",
    "    queriedDataHash = {}\n",
    "\n",
    "    for row in inputData:\n",
    "        inputDataHash[row['id']] = row\n",
    "    for row in queriedData:\n",
    "        queriedDataHash[row['id']] = row\n",
    "\n",
    "    idsInInputData = [d['id'] for d in inputData]\n",
    "    for row in queriedData:\n",
    "        if row['id'] not in idsInInputData:\n",
    "            data.append(row)\n",
    "\n",
    "    for row in data:\n",
    "        if not row['width']:\n",
    "            if row['id'] in queriedDataHash and queriedDataHash[row['id']]['width']:\n",
    "                row['width'] = queriedDataHash[row['id']]['width']\n",
    "            elif row['id'] in inputDataHash and inputDataHash[row['id']]['width']:\n",
    "                row['width'] = inputDataHash[row['id']]['width']\n",
    "        if not row['height']:\n",
    "            if row['id'] in queriedDataHash and queriedDataHash[row['id']]['height']:\n",
    "                row['height'] = queriedDataHash[row['id']]['height']\n",
    "            elif row['id'] in inputDataHash and inputDataHash[row['id']]['width']:\n",
    "                row['height'] = inputDataHash[row['id']]['height']\n",
    "    \n",
    "    writeData(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Get (missing) image sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the original image size is not specified, call the IIIF Image API to read the size from the JSON rsponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28127/28127 [00:08<00:00, 3266.45it/s]  \n"
     ]
    }
   ],
   "source": [
    "for row in tqdm(data):\n",
    "    if not row['width'] or not row['height']:\n",
    "        uri = row['image'] + '/info.json'\n",
    "        try:\n",
    "            with urllib.request.urlopen(uri) as url:\n",
    "                manifestData = json.loads(url.read().decode())\n",
    "                \n",
    "        except:\n",
    "            print(\"Could not open\", uri)\n",
    "            next\n",
    "        row['width'] = manifestData['width']\n",
    "        row['height'] = manifestData['height']\n",
    "        writeData(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write data to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "writeData(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Download images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the images that do not yet exist in the image folder. The images will be downloaded resized to a width of 1024 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    Path(config['imageDirectory']).mkdir(parents=True, exist_ok=True)\n",
    "except:\n",
    "    raise Exception(\"Could not add/access folder\", config['imageDirectory'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▊    | 16483/28127 [00:00<00:00, 86143.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not download nb-812808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 24738/28127 [00:11<00:02, 1610.45it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not download nb-815037\n",
      "Could not download nb-815050\n",
      "Could not download nb-815054\n",
      "Could not download nb-815062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 24761/28127 [00:34<00:07, 422.45it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not download nb-815093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 88%|████████▊ | 24763/28127 [00:39<00:10, 334.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not download nb-815097\n",
      "Could not download nb-815102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 24774/28127 [00:51<00:15, 210.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not download nb-815125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 88%|████████▊ | 24833/28127 [00:56<00:19, 168.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not download nb-815670\n",
      "Could not download nb-822350\n",
      "Could not download nb-838092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 25755/28127 [01:13<00:20, 114.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not download nb-838152\n",
      "Could not download nb-838155\n",
      "Could not download nb-838157\n",
      "Could not download nb-838160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 25759/28127 [01:36<00:39, 60.06it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not download nb-838162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 92%|█████████▏| 25760/28127 [01:42<00:46, 51.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not download nb-838164\n",
      "Could not download nb-838166\n",
      "Could not download nb-838168\n",
      "Could not download nb-838170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 25764/28127 [02:04<01:23, 28.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not download nb-838172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 25764/28127 [02:06<00:11, 204.36it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-7009a09171ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m'image'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Content-Type'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mretries\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mmaxRetries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0;31m# Try again if no image comes back\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_redirects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mretries\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "maxRetries = 5\n",
    "for row in tqdm(data):\n",
    "    filename = path.join(config['imageDirectory'], row['id'] + '.jpg')\n",
    "    if not path.exists(filename):\n",
    "        url = row['image'] + '/full/1024,/0/default.jpg'\n",
    "        r = requests.get(url, allow_redirects = True)\n",
    "        retries = 1\n",
    "        while not 'image' in r.headers['Content-Type'] and retries <= maxRetries:\n",
    "            # Try again if no image comes back\n",
    "            time.sleep(1)\n",
    "            r = requests.get(url, allow_redirects = True)\n",
    "            retries += 1\n",
    "        if retries >= maxRetries:\n",
    "            print(\"Could not download\", row['id'])\n",
    "        else:\n",
    "            with open(filename, 'wb') as f:\n",
    "                f.write(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Detect Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extension = 50\n",
    "\n",
    "for row in tqdm(data):\n",
    "    if not row['documentCoordinates'] or len(row['documentCoordinates']) == 0:\n",
    "        filename = path.join(config['imageDirectory'], row['id'] + '.jpg')\n",
    "        # Load image\n",
    "        image = cv2.imread(filename)\n",
    "    \n",
    "        # Extend image border\n",
    "        extendedImage = cv2.copyMakeBorder(image,extension,extension,extension,extension,cv2.BORDER_REPLICATE)\n",
    "        \n",
    "        # Convert to grayscale and blur\n",
    "        gray = cv2.cvtColor(extendedImage.copy(), cv2.COLOR_RGB2GRAY)\n",
    "        gray = cv2.blur(gray, (5, 5))\n",
    "        \n",
    "        # Binarise\n",
    "        ret, thresh = cv2.threshold(gray,0,255,cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)\n",
    "        plt.imshow(Image.fromarray(thresh))\n",
    "        \n",
    "        # Detect contours\n",
    "        contours, hierarchy = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        img_contours = np.zeros(extendedImage.shape)\n",
    "        cv2.drawContours(img_contours, contours, -1, (0,255,0), 3)\n",
    "        \n",
    "        # Identify biggest contour\n",
    "        areas = [cv2.contourArea(c) for c in contours]\n",
    "        max_index = np.argmax(areas)\n",
    "        cnt=contours[max_index]\n",
    "        x,y,w,h = cv2.boundingRect(cnt)\n",
    "        img = np.zeros(extendedImage.shape)\n",
    "        cv2.rectangle(img,(x,y),(x+w,y+h),(0,255,0),2)\n",
    "        \n",
    "        # Remove frame\n",
    "        x = x - extension if x > extension else 0\n",
    "        y = y - extension if y > extension else 0\n",
    "        \n",
    "        # Upscale to original size\n",
    "        row['documentCoordinates'] = \"%d,%d,%d,%d\" % (int( x / 1024 * int(row['width'])),\n",
    "                                                      int( y / 1024 * int(row['height'])),\n",
    "                                                      int( w / 1024 * int(row['width'])),\n",
    "                                                      int( h / 1024 * int(row['height'])))\n",
    "\n",
    "        # Store coordinates in data after every prediction\n",
    "        writeData(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Output as CIDOC-CRM RDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output as a Trig file that can be displayed and edited in the Mirador component of ResearchSpace & Metaphacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "namespaces = \"\"\"\n",
    "PREFIX Platform: <http://www.metaphacts.com/ontologies/platform#> \n",
    "PREFIX User: <http://www.metaphacts.com/resource/user/> \n",
    "PREFIX xsd: <http://www.w3.org/2001/XMLSchema#> \n",
    "PREFIX crmdig: <http://www.ics.forth.gr/isl/CRMdig/> \n",
    "PREFIX rso: <http://www.researchspace.org/ontology/> \n",
    "PREFIX prov: <http://www.w3.org/ns/prov#> \n",
    "PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> \n",
    "PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "PREFIX ldp: <http://www.w3.org/ns/ldp#> \n",
    "PREFIX crm: <http://www.cidoc-crm.org/cidoc-crm/>\n",
    "\"\"\"\n",
    "\n",
    "static = \"\"\"\n",
    "\n",
    "<https://platform.swissartresearch.net/imageRegions> {\n",
    "    <https://resource.swissartresearch.net/type/imageRegion> a crm:E55_Type ;\n",
    "    rdfs:label \"Image Region\" ;\n",
    "    crm:P3_has_note \"A region defining the visual image represented within a digital image. For example, the region denotes the visual item that is reproduced on a document which is photographed.\".\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "regionTemplate = Template('''<$uri/container/context> {\n",
    "  Platform:formContainer ldp:contains <$uri/container> .\n",
    "  \n",
    "  <$uri>\n",
    "    a crmdig:D35_Area, rso:EX_Digital_Image_Region;\n",
    "    crmdig:L49_is_primary_area_of <$iiifImage>;\n",
    "    crm:P33_used_specific_technique <https://github.com/swiss-art-research-net/bso-image-segmentation> ;\n",
    "    rso:boundingBox \"xywh=$x,$y,$w,$h\";\n",
    "    rso:displayLabel \"image\";\n",
    "    rso:viewport \"xywh=0,0,0,0\";\n",
    "    rdf:value \"<svg xmlns='http://www.w3.org/2000/svg'><path xmlns=\\\\\"http://www.w3.org/2000/svg\\\\\" d=\\\\\"M${x0},${y0}l${halfW},0l0,0l${halfW},0l 0,${halfH}l 0,${halfH}l -${halfW},0l -${halfW},0l 0,-${halfH}z\\\\\" data-paper-data=\\\\\"{&quot;defaultStrokeValue&quot;:1,&quot;editStrokeValue&quot;:5,&quot;currentStrokeValue&quot;:1,&quot;rotation&quot;:0,&quot;deleteIcon&quot;:null,&quot;rotationIcon&quot;:null,&quot;group&quot;:null,&quot;editable&quot;:true,&quot;annotation&quot;:null}\\\\\" id=\\\\\"rectangle_e880ad36-1fef-4ce3-835d-716ba7db628a\\\\\" fill-opacity=\\\\\"0\\\\\" fill=\\\\\"#00bfff\\\\\" fill-rule=\\\\\"nonzero\\\\\" stroke=\\\\\"#00bfff\\\\\" stroke-width=\\\\\"4.04992\\\\\" stroke-linecap=\\\\\"butt\\\\\" stroke-linejoin=\\\\\"miter\\\\\" stroke-miterlimit=\\\\\"10\\\\\" stroke-dasharray=\\\\\"\\\\\" stroke-dashoffset=\\\\\"0\\\\\" font-family=\\\\\"none\\\\\" font-weight=\\\\\"none\\\\\" font-size=\\\\\"none\\\\\" text-anchor=\\\\\"none\\\\\" style=\\\\\"mix-blend-mode: normal\\\\\"/></svg>\" .\n",
    "  \n",
    "  <$uri/container>\n",
    "    a ldp:Resource, prov:Entity;\n",
    "    prov:generatedAtTime \"$dateTime\"^^xsd:dateTime;\n",
    "    prov:wasAttributedTo User:admin .\n",
    "}\n",
    "\n",
    "<https://platform.swissartresearch.net/imageRegions> {\n",
    "    <$uri> crm:P2_has_type <https://resource.swissartresearch.net/type/imageRegion> .\n",
    "}\n",
    "\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:00<00:00, 13503.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not detect coordinates in 1 images:\n",
      "SFF_D_1010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dateTime = datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S+00:00z\")\n",
    "\n",
    "output = namespaces + static\n",
    "\n",
    "missingDocumentCoordinates = []\n",
    "\n",
    "for row in tqdm(data):\n",
    "    if row['documentCoordinates'] is None:\n",
    "        missingDocumentCoordinates.append(row)\n",
    "        continue\n",
    "        \n",
    "    docCoords = row['documentCoordinates'].split(',')\n",
    "    \n",
    "    if len(docCoords) < 4:\n",
    "        missingDocumentCoordinates.append(row)\n",
    "        continue\n",
    "\n",
    "    x = int(docCoords[0])\n",
    "    y = int(docCoords[1])\n",
    "    w = int(docCoords[2])\n",
    "    h = int(docCoords[3])\n",
    "\n",
    "#     edges = {\n",
    "#         \"topLeft\": (x, y),\n",
    "#         \"topRight\": (x + w, y),\n",
    "#         \"bottomRight\": (x + w, y + h),\n",
    "#         \"bottomLeft\": (x, y + h)\n",
    "#     }\n",
    "    iiifImage = row['image']\n",
    "    identifier = str(uuid.uuid3(uuid.NAMESPACE_DNS, iiifImage))\n",
    "    uri = \"https://resource.swissartresearch.net/digitalobject/\" + identifier\n",
    "#     x0 = edges['topLeft'][0]\n",
    "#     y0 = edges['topLeft'][1]\n",
    "#     x1 = edges['bottomRight'][0]\n",
    "#     y1 = edges['bottomRight'][1]\n",
    "#     x = x0\n",
    "#     y = y0\n",
    "#     w = x1 - x0\n",
    "#     h = y1 - y0\n",
    "    output += regionTemplate.substitute(\n",
    "        uri=uri,\n",
    "        iiifImage=iiifImage,\n",
    "        x=int(x),\n",
    "        y=int(y),\n",
    "        w=int(w),\n",
    "        h=int(h),\n",
    "        x0=x0,\n",
    "        y0=y0,\n",
    "        halfW=float(w/2),\n",
    "        halfH=float(h/2),\n",
    "        dateTime=dateTime\n",
    "    )\n",
    "\n",
    "# Write summary of missing corodinates\n",
    "if len(missingDocumentCoordinates) > 0:\n",
    "    print(\"Could not detect coordinates in %d images:\" % len(missingDocumentCoordinates))\n",
    "    print('\\n'.join([d['id'] for d in missingDocumentCoordinates]))\n",
    "    \n",
    "filename = path.join(config['trigFile'])\n",
    "with open(filename, 'w') as f:\n",
    "    f.write(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
